{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config\n",
    "from easydict import EasyDict\n",
    "import yaml\n",
    "\n",
    "def load_config(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return EasyDict(yaml.safe_load(f))\n",
    "\n",
    "config_file = './configs/vector_attention.yml'\n",
    "config = load_config(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chem import read_sdf\n",
    "from dataset.featurizer import featurize_mol\n",
    "from dataset.molgeom import torchify_dict\n",
    "from torch_geometric.data import Data, Batch\n",
    "mols = read_sdf('./data/mol.sdf')\n",
    "\n",
    "mol_feat_dicts = [featurize_mol(mol) for mol in mols]\n",
    "mols_feats = [torchify_dict(mol_feat_dict) for mol_feat_dict in mol_feat_dicts]\n",
    "mol_datas = [\n",
    "    Data(x=mols_feat['atom_feature'],pos=mols_feat['pos'],\n",
    "    edge_index=mols_feat['bond_index'], edge_attr=mols_feat['bond_feature'], element=mols_feat['element'])\n",
    "    for mols_feat in mol_feat_dicts]\n",
    "\n",
    "batch = Batch.from_data_list(mol_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gvat import *\n",
    "import torch\n",
    "class VectorAttention(Module):\n",
    "    # SO(3) version \n",
    "    # Global structure and local structure\n",
    "    # The vector \\times vector attention remains equivariant: Frobnenius norm, i.e., perform summation on the last dimension twice\n",
    "    # Equivaraint example could be referred to the AttentionEdges in ./models/tri_attention.py\n",
    "    def __init__(self, node_dim=45, edge_dim=5, node_hiddens=[256, 64], edge_hidden=64, key_channels=128, num_heads=4, num_blocks=4, k=32, cutoff=10.0):\n",
    "        super().__init__()\n",
    "        self.node_hiddens = node_hiddens\n",
    "        self.edge_hidden = edge_hidden\n",
    "        self.key_channels = key_channels  # not use\n",
    "        self.num_heads = num_heads  # not use\n",
    "        self.num_blocks = num_blocks\n",
    "        self.k = k\n",
    "        self.cutoff = cutoff\n",
    "        self.node_dim = node_dim\n",
    "        self.edge_dim = edge_dim\n",
    "        self.atom_sca_mapper = Linear(node_dim, node_hiddens[0])\n",
    "        self.atom_vec_mapper = VNLinear(1, node_hiddens[1])\n",
    "        self.interactions = ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            block = AttentionInteractionBlockVN(\n",
    "                node_hiddens=node_hiddens,\n",
    "                edge_hidden=edge_hidden,\n",
    "                num_edge_types=edge_dim,\n",
    "                key_channels=key_channels,\n",
    "                num_heads=num_heads,\n",
    "                cutoff = cutoff\n",
    "            )\n",
    "            self.interactions.append(block)\n",
    "\n",
    "    def forward(self, node_attr, pos, edge_index, edge_feature):\n",
    "    \n",
    "        if len(pos.shape) != 3:\n",
    "            vector_feature = pos.unsqueeze(1) # torch.Size([batch, 1, 3])\n",
    "        else:\n",
    "            vector_feature = pos\n",
    "            \n",
    "        edge_vector = pos[edge_index[0]] - pos[edge_index[1]]\n",
    "        atom_sca_hidden0 = self.atom_sca_mapper(node_attr)\n",
    "        atom_vec_hidden0 = self.atom_vec_mapper(vector_feature)\n",
    "        # There are two possible attention could be designed\n",
    "        # First, Edge vector attention with node features\n",
    "        # second, edge scalar attention with \n",
    "        # self-attention on each atom\n",
    "\n",
    "        h = [atom_sca_hidden0, atom_vec_hidden0]\n",
    "        for interaction in self.interactions:\n",
    "            delta_h = interaction(h, edge_index, edge_feature, edge_vector)\n",
    "            h[0] = h[0] + delta_h[0]\n",
    "            h[1] = h[1] + delta_h[1]\n",
    "        # global could be incorporated here\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvat = VectorAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sca_feature, vec_feature = gvat(batch.x, batch.pos, batch.edge_index, batch.edge_attr)\n",
    "# perform graph attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1255, 0.1244, 0.1127, 0.1149, 0.1222, 0.1163, 0.1121, 0.1120, 0.1105,\n",
       "        0.1119], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_scatter import scatter_mean\n",
    "# for example, perform the average of node features of each graph\n",
    "\n",
    "prediction_layer = Linear(sca_feature.shape[-1],1)\n",
    "prediction = prediction_layer(sca_feature).squeeze(dim=-1) # squeeze to the 1-dim vector\n",
    "\n",
    "\n",
    "scatter_mean(prediction, batch.batch)\n",
    "# then the loss could be obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mics import get_optimizer, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the use of scheduler and optimizer could be found at ResGen: https://github.com/HaotianZhangAI4Science/ResGen\n",
    "optimizer = get_optimizer(config.train.optimizer,gvat)\n",
    "scheduler = get_scheduler(config.train.scheduler,optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('carbon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cb457098628399098f8244ea6d862b61e5b409c4fe20c91d3202c562013c713"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
